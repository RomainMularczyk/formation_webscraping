{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aspects légaux et éthiques du webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Aspects légaux du webscraping\n",
    "\n",
    "* **Une zone grise juridique :**\n",
    "    * Pourtant, une **définition précise** de l'activité : le webscraping est une technique d'extraction automatique du contenu de sites web (donc du contenu de code HTML)\n",
    "    * Selon le contexte, différentes normes s'appliquent (propriété intellectuelle, données personnelles, etc.)\n",
    "    * De nombreuses réflexions sont en cours autour de solutions comme les CGR (conditions générales de réutilisation) pour encadrer la réutilisation de données collectées sur les sites web : celles-ci s'appuieraient notamment sur des licences (précisant les droits et permissions) directement attachées aux données collectées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Aspects légaux du webscraping\n",
    "\n",
    "* **D'une situation à l'autre :**\n",
    "    * Le **contexte est important** lorsqu'on parle de webscraping, selon les données que l'on souhaite collecter, selon ce qu'on souhaite en faire, il faut prêter attention à différents aspects\n",
    "    * Ce qui importe avant tout est l'**usage des données** qui est fait suite à la collecte\n",
    "    * Et ce qui compte, par ailleurs, est la **façon d'extraire les données**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Aspects légaux du webscraping\n",
    "\n",
    "* **Une pratique pas forcément bien vue et légitimement discutable :**\n",
    "\n",
    "    * Il s'agit de copier des informations collectées, produites par un tiers (cela peut donc être vu comme une forme de vol ou, du moins, d'appropriation)\n",
    "    * Pour autant, les pages web sont **librement accessibles** et leur **code source ouvert** (peut-être voler quelque chose qui est librement accessible ?)\n",
    "    * Cela rejoint aussi une philosophie sous-jacente des technologies du web et plus largement, de l'histoire d'internet (ouverture, gratuité, libre accès, neutralité des réseaux, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Aspects légaux du webscraping\n",
    "\n",
    "* **Les cas d'affaires concernent d'abord des entreprises qui tirent un profit des données collectées :**\n",
    "    * Il existe depuis 2015 en France **une loi sur le \"vol de donnée\"** (modifie l'article 323-3 du Code pénal) qui interdit \"d'extraire, de détenir, de reproduire, de transmettre frauduleusement les données d'un système de traitement automatisé de données\"\n",
    "    * Il existe divers enjeux autour du webscraping, mais le plus conflictuel reste ceux qui touchent à la **propriété intellectuelle**, à la concurrence et à l'appropriation de données décisives sur certains marchés\n",
    "    * Si on récupère des données sur un site web, il faut au moins les modifier substanciellement ou en tirer une nouvelle information pour considérer que l'on est pas dans un cas de **plagiat**\n",
    "    * Lorsqu'on utilise ces techniques dans le cadre d'une recherche, on se situe, a priori, dans ce cadre\n",
    "    * On fera alors attention à l'usage fait des **données collectées**, notamment celles qui constituent des données personnelles (et donc soumises au RGPD), mais également aux **données protégées par un droit d'auteur** (image, musique, vidéo, etc.)\n",
    "    * Ceci étant, le droit d'auteur protège uniquement les **créations originales** et pas les faits, les statistiques (et donc les métadonnées)\n",
    "    * Le **droit d'auteur** ne protège pas contre la collecte de données mais uniquement contre la publication et la mise à disposition : a priori, on peut donc également collecter des données protégées par un droit d'auteur si c'est dans un unique but d'analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Eléments de culture numérique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Eléments de culture numérique\n",
    "\n",
    "* **On peut se référer à ce que nous disent les sites web eux-mêmes :**\n",
    "    * A la racine de chaque site web, on retrouve régulièrement un fichier qui se nomme `robots.txt`\n",
    "    * Il permet de définir ce que les robots (y compris les scrapers donc) sont autorisés ou non à faire sur le site web\n",
    "    * ```User-agent: *\n",
    "      Disallow: /``` : aucun robot n'est autorisé à parcourir quelconque page de ce site web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Eléments de culture numérique\n",
    "\n",
    "* **On ne doit pas accéder à un espace qui est explicitement protégé et dont l'accès est restreint :**\n",
    "    * Lorsqu'une page web est protégée par un **mot de passe**, il s'agit d'une restriction explicite qui nous place en dehors des bornes légales si on tente de la franchir\n",
    "    * De la même façon, contourner les limites posées par les dispositifs comme les fameux **CAPTCHA** destinés à exclure les robots, constitue une infraction à une restriction explicite\n",
    "    * Certains sites web affichent explicitement qu'ils refusent être parcourus par des robots (en faisant accepter, par exemple, des **termes d'usage**) : dans ce cas, on contreviendra à cette disposition si on souhaite tout de même le scraper sans obtenir le consentement des administrateurs du site en question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Eléments de culture numérique\n",
    "\n",
    "* **On peut être tenu responsable d'impacter le fonctionnement des serveurs requêtés :**\n",
    "    * Une pratique mal maîtrisée de scraping peut **endommager ou contribuer à endommager les serveurs** auxquels des requêtes HTTP sont envoyées : dans ce cas, on peut s'exposer à des poursuites de la part des administrateurs ou propriétaires de ces serveurs (exemple des attaques de type **DDOS** ou \"attaques par déni de service\")\n",
    "    * Avant d'endommager (ce qui est un peu extrême), on pourra éventuellement contribuer à **limiter l'accès d'autres utilisateurs** aux ressources que le serveur rend accessibles\n",
    "    * Enfin, si on écrit un morceau de code, c'est que l'on sait ce que l'on est en train de faire : on est alors responsable des actions de ce script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Bonnes pratiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3. Bonnes pratiques\n",
    "\n",
    "* **Avant de se lancer tête baissée :**\n",
    "    * On peut vérifier si le site web que l'on veut scraper ne **met pas à disposition une API** : cela constitue un accès réglementé et donc légal et peut, par ailleurs, parfois grandement faciliter les opérations de collecte de données\n",
    "    * Si aucune API n'est disponible ou ne permet pas de récupérer les informations qui nous intéressent, on commencera alors à modéliser le schéma des données qui nous intéresse et à cibler les pages d'intérêt (plutôt que de parcourir toutes les pages du site web et de récupérer plus d'information que nécessaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3. Bonnes pratiques\n",
    "\n",
    "* **Pour éviter de surcharger les serveurs :**\n",
    "    * On veillera à scraper **lentement** : les instructions dans Python sont exécutées rapidement (par exemple, on peut parcourir une boucle sur 10 000 éléments en moins d'une seconde), mais les serveurs ne peuvent pas répondre aussi rapidement\n",
    "    * Pour cela, on peut utiliser l'instruction `time.sleep(5)` pour demander à notre script de faire une pause de 5 secondes entre chaque tour de boucle, par exemple\n",
    "    * Pour éviter de ralentir les requêtes d'autres utilisateurs, on peut lancer une opération de scraping très longue durant la nuit plutôt que la journée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Références :**\n",
    "* Ryan Mitchell, *Web scraping with Python. Collecting more data from the modern web*, Sebastopol, O'Reilly, 2018.\n",
    "* Thomas Saint-Aubin et Charles Leconte, \"Droit : Comment faire du scraping de données en toute légalité ?\", 2020, URL : https://www.archimag.com/univers-data/2020/01/14/droit-comment-web-scraping-donnees-legalite, consulté le 21 février 2021."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
