{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table des matières :\n",
    "\n",
    "* <a href=\"#Introduction\">1. Introduction</a>\n",
    "* <a href=\"#Envoyer une requête HTTP avec Python\">2. Envoyer une requête HTTP avec Python</a>\n",
    "* <a href=\"#Parser une page web avec BeautifulSoup\">3. Parser une page web avec <code>BeautifulSoup</code></a>\n",
    "* <a href=\"#Avec une vraie page web...\">4. Avec une vraie page web...</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div id=\"Introduction\">1. Introduction</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons peut-être par définir et expliciter en quoi consiste le **web scraping**. On peut résumer cela en trois points, en empruntant une définition simple proposée par Kimberly Fessel :\n",
    "* Collecter de l'information depuis des sites web\n",
    "* Parser son contenu dans un format structuré que l'on puisse représenter et extraire de la même façon (CSV, JSON, XML, etc.)\n",
    "* Faire le tout de façon entièrement automatisée\n",
    "\n",
    "Se pose également une question plus pratique : pourquoi apprendre à scraper des pages web ? Il y a plusieurs raisons à cela, mais la question a d'autant plus de sens lorsqu'on croise de nombreuses ressources et outils proposant de scraper des sites web sans plonger davantage dans le code. Listons-en quelques-une :\n",
    "* Le web scraping permet de gagner énormément de temps : si l'on souhaite constituer des jeux de données massifs, il devient quasiment inenvisageable de constituer manuellement des tableaux de données de plus de quelques milliers de lignes\n",
    "* C'est une technique qui permet, non pas seulement d'accéder à des données, mais de se constituer soi-même ses propres jeux de données en croisant, associant diverses sources\n",
    "* Elle est une façon de se familiariser avec la construction de données structurées et offre de nombreux concepts et façons de faire transposables à de nombreuses autres opérations faisant intervenir des données structurées (tri de fichiers, catégorisation, tagging, cartographie, etc.)\n",
    "* Enfin, elle constitue un outil d'exploration qui permet de mieux appréhender le fonctionnement des infrastructures numériques contemporaines, en particulier de l'internet et du web\n",
    "\n",
    "On l'aura compris, le **web scraping** vous dote de **super pouvoirs**, mais qui dit grands pouvoirs, dit aussi grandes responsabilités. Ainsi, nous évoquerons les aspects éthiques et légaux du **web scraping** dans le module final de cette formation.\n",
    "\n",
    "Dans ce module, nous passons à la pratique en explorant nos premiers cas concrets d'application de Python pour le **web scraping**. Avant d'entamer ce module, il est vivement conseillé de revoir :\n",
    "* Le module 1 et d'être à l'aise avec le fonctionnement de langages à balises comme HTML\n",
    "* Les module 2 et 3 et d'être à l'aise avec les pratiques élémentaires de programmation avec Python (structures de données, boucles, conditions, etc.)\n",
    "\n",
    "Au cours de ce module, nous apprendrons :\n",
    "* A envoyer une requête HTTP à un serveur à l'aide du module `requests`\n",
    "* A parser une page HTML simple à l'aide du module `BeautifulSoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div id=\"Envoyer une requête HTTP avec Python\">2. Envoyer une requête HTTP avec Python</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accéder à une page web suppose avant tout de la récupérer : pour cela, on utilise le **protocole HTTP** qui nous permet de former des **requêtes HTTP**. L'objectif ici n'est pas de rentrer dans le détail, d'autant que nous avons pu voir quelques aspects élémentaires durant le premier module, que de comprendre simplement les différentes étapes qui structurent un projet de **web scraping**.\n",
    "\n",
    "Python dispose d'une **bibliothèque** nommée `requests` qui permet de construire et d'envoyer des **requêtes HTTP**. Il existe d'autres façons de faire, mais `requests` a l'avantage d'être simple d'utilisation et d'être très flexible et pourra vous être utile pour bien d'autres projets.\n",
    "\n",
    "Voyons toute de suite comment récupérer le contenu d'une page web à l'aide de `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html lang=\"fr-FR\">\n",
      "  <head>\n",
      "      <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\n",
      "    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\n",
      "    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=\n",
      "    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);\n",
      "    })(window,document,'script','dataLayer','GTM-KNCB8CD');</script>\n",
      "  \n",
      "  <meta charset=\"utf-8\">\n",
      "  <meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\">\n",
      "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" X-Content-Type-Options=\"nosniff\">\n",
      "\n",
      "  <link href=\"https://www.msh-lse.fr/wp-content/themes/pamplemousse/dist/images/favicons/apple-touch-icon-57x57.png\" rel=\"apple-touch-icon-precomposed\" sizes=\"57x57\">\n",
      "  <link href=\"https://www.msh-lse.fr/wp-content/themes/pamplemousse/dist/images/favicons/apple-touch-icon-114x114.png\" rel=\"apple-tou\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"https://www.msh-lse.fr/\")\n",
    "# Affichons simplement les 1000 premiers caractères car la page est très longue\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons le temps de commenter ces quelques lignes.\n",
    "\n",
    "Comme avec toute **bibliothèque** de code, il s'agit dans un premier temps de l'importer.\n",
    "\n",
    "Ensuite, on peut utiliser l'**objet** `requests` sur lequel on peut appeler différentes méthodes. Dans notre cas, nous allons utiliser uniquement la **méthode** `.get()` car nous ne nous intéressons qu'à la récupération de pages web. Cette **méthode** `.get()` prend plusieurs **paramètres** et doit contenir, au minimum, un **paramètre** indiquant l'**adresse URL** qui nous intéresse.\n",
    "\n",
    "`requests` renvoie un objet de type `requests.models.Response`. C'est la **réponse** que le serveur nous a renvoyé lorsque nous avons demandé notre **URL** dans la **requête HTTP** que nous lui avons envoyé.\n",
    "\n",
    "Cet **objet** dispose de différentes **propriétés** : la première qui nous intéresse ici est celle nommée `.text`. Elle permet de récupérer l'intégralité du code source de page web (donc du code HTML). A partir de ce code, nous allons pouvoir récupérer les éléments qui nous intéressent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "print(type(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div id=\"Un peu de pratique : Construire et envoyer plusieurs requêtes HTTP\">2. 2. Un peu de pratique : Construire et envoyer plusieurs requêtes HTTP</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\"><strong>Premier exercice : </strong>Dans cet exercice, nous allons créer un seul modèle de requête à l'aide de <code>requests</code> à partir duquel nous effectuerons plusieurs requêtes différentes pour récupérer le code HTML des pages web suivantes :\n",
    "<ul>\n",
    "    <li><a href=\"http://www.reddit.com/r/HTML\">http://www.reddit.com/r/HTML</a></li>\n",
    "    <li><a href=\"http://www.reddit.com/r/python\">http://www.reddit.com/r/python</a></li>\n",
    "    <li><a href=\"http://www.reddit.com/r/webscraping\">http://www.reddit.com/r/webscraping</a></li>\n",
    "</ul>\n",
    "L'objectif est de partir de la liste suivante, qui contient le nom des sub-reddit correspondants : <code>subreddit = [\"html\", \"python\", \"webscraping\"]</code>. On stockera la réponse dans un dictionnaire qui fera correspondre le sub reddit avec le contenu de la page. Si vous vous sentez intrépide, vous pouvez encapsuler le tout dans une fonction pour avoir un code propre et réutilisable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\"><strong>Utiliser la boîte à outil Python pour créer un pipeline de requêtes HTTP :</strong>\n",
    "<br>\n",
    "<br>\n",
    "<pre><code>import requests\n",
    "\n",
    "subreddit = [\"html\", \"python\", \"webscraping\"]\n",
    "content = {}\n",
    "\n",
    "def req(subreddit):\n",
    "    r = requests.get(f\"http://www.reddit.com/r/{subreddit}\")\n",
    "    return r.text\n",
    "\n",
    "for sub in subreddit:\n",
    "    content[sub] = req(sub)</code></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div id=\"Parser une page web avec BeautifulSoup\">3. Parser une page web avec <code>BeautifulSoup</code></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous savons comment récupérer le contenu d'une page web, il ne nous reste plus qu'à apprendre comment **parser** une page web !\n",
    "\n",
    "**Parser** un document signifie le parcourir intégralement et le filtrer afin de ne récupérer que les parties qui nous intéresse. Cela peut être utile lorsque l'on souhaite extraire des données, comme dans notre cas, mais également lorsqu'on souhaite modifier un document, compter des occurrences, etc.\n",
    "\n",
    "`BeautifulSoup` nous aide grandement pour cela. Il y a mille et une façons de s'y prendre pour récupérer les informations qui nous intéressent mais le plus important est d'abord de savoir ce que l'on peut faire et ce que l'on ne peut pas faire. Pour cela, pas de secret, on se dirige à nouveau vers la documentation : https://www.crummy.com/software/BeautifulSoup/bs4/doc/. La chance avec `BeautifulSoup`, c'est que la documentation est particulièrement bien faite et très riche : on trouve de nombreux exemples pour comprendre, en contexte, comment utiliser les méthodes de la bibliothèque.\n",
    "\n",
    "Commençons par voir un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li class=\"email\" id=\"personnel\">wsenter0@blogspot.com</li>, <li class=\"email\" id=\"professionnel\">wsenter0@cyberchimps.com</li>, <li class=\"email\" id=\"personnel\">abroddle1@tripadvisor.com</li>, <li class=\"email\" id=\"professionnel\">abroddle1@wisc.edu</li>, <li class=\"email\" id=\"personnel\">aboothebie2@e-recht24.de</li>, <li class=\"email\" id=\"professionnel\">aboothebie@irs.gov</li>, <li class=\"email\" id=\"personnel\">meast3@webeden.co.uk</li>, <li class=\"email\" id=\"personnel\">meast3@sohu.com</li>, <li class=\"email\" id=\"personnel\">cmcging4@so-net.ne.jp</li>, <li class=\"email\" id=\"professionnel\">cmcging4@people.com.cn</li>]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"data/module_1-carnet.html\", \"r\") as file:\n",
    "    html = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "print(soup.find_all(\"li\", attrs={\"class\": \"email\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de travailler à partir d'une vraie page web, commençons par un petit échauffement avec un fichier HTML très simple (et mal formé, soit dit en pensant, si on se souvient des éléments vus dans le premier module).\n",
    "\n",
    "Le fonctionnement est assez simple : pour **parser** une page web (donc un document HTML) ou un fichier XML avec `BeautifulSoup`, on appelle la fonction `BeautifulSoup()` à laquelle on passe au moins deux paramètres :\n",
    "* Le texte du fichier HTML ou XML\n",
    "* Un parser (il en existe plusieurs, on pourra d'ailleurs se référer à la documentation de `BeautifulSoup` pour avoir plus de détails (https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser), ici nous utiliserons le parser par défaut\n",
    "\n",
    "Une fois que l'on a parsé notre fichier, on peut utiliser toute une série de **méthodes** pour parcourir la structure arborescente de notre fichier HTML ou XML.\n",
    "\n",
    "Dans notre cas, nous utilisons la **méthode** `.find_all()` qui permet de rechercher toutes les balises qui matchent les conditions passées comme **arguments**. On peut passer plusieurs arguments à la **méthode** `.find_all()` :\n",
    "* Le tag à rechercher (`div`, `li`, `a`, etc.)\n",
    "* Un attribut et sa valeur associée dans un dictionnaire que l'on passe au paramètre `attrs`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\"><strong>Deuxième exercice :</strong> A l'aide de <code>BeautifulSoup</code>, récupérez dans un dictionnaire toutes les adresses email des membres du carnet d'adresse. Pour rappel, vous pouvez importer le carnet d'adresse <code>module_1-carnet.html</code> depuis le dossier <code>data</code>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\"><strong>Capturer les erreurs renvoyées par BeautifulSoup</strong>\n",
    "<br>\n",
    "La difficulté dans cet exercice repose sur le fait que BeautifulSoup lève des erreurs lorsqu'il n'y a pas d'attributs pour une balise spécifique. Il faut penser à capturer les erreurs pour que le script ne s'arrête pas en cours de route.\n",
    "<br>\n",
    "<br>\n",
    "<pre><code>from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"data/module_1-carnet.html\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "soup = BeautifulSoup(data, \"html.parser\")\n",
    "carnet = soup.div\n",
    "\n",
    "dict_mail = {}\n",
    "name = \"\"\n",
    "\n",
    "for child in carnet.children:\n",
    "    try:\n",
    "        name = child.li.get_text()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    try:\n",
    "        dict_mail[name] = child.find_all(\"li\", attrs={\"class\": \"email\"})\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(dict_mail)</code></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div id=\"Avec une vraie page web...\">4. Avec une vraie page web...</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons maintenant un exemple concret. Nous allons tenter de récupérer les informations de l'historique de l'émission \"Le cours de l'histoire\" sur le site internet de France Culture.\n",
    "\n",
    "Commençons par nous connecter à la page à partir de laquelle on souhaite travailler et affichons les 100 premiers caractères de la page web pour vérifier que l'on a bel et bien récupéré une page web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n    <html lang=\"fr\" prefix=\"og: http://ogp.me/ns#\">\\n    <head>\\n                     '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"https://www.franceculture.fr/emissions/le-cours-de-lhistoire\")\n",
    "r.text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous pouvons créer une **instance** de `BeautifulSoup` en passant la page web au format texte en que l'on a récupéré à l'aide de `requests` en **paramètre**.\n",
    "\n",
    "On pensera également à ajouter en **second paramètre** un **parser** pour parcourir la page web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, il nous faut être un peu malins et essayer de comprendre, en consultant les **outils de développement de notre navigateur préféré**, à partir de quelle **balise** on peut opérer pour récupérer les données qui nous intéressent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources et références\n",
    "\n",
    "https://towardsdatascience.com/5-strategies-to-write-unblock-able-web-scrapers-in-python-5e40c147bdaf\n",
    "\n",
    "https://medium.com/hackernoon/how-to-scrape-a-website-without-getting-blacklisted-271a605a0d94\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kimberly Fessel : \"It's officially legal so let's scrape the web\", URL : https://www.youtube.com/watch?v=RUQWPJ1T6Zc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
