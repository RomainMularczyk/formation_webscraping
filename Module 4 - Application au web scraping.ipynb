{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table des matières :\n",
    "\n",
    "* <a href=\"#Introduction\">1. Introduction</a>\n",
    "* <a href=\"#Envoyer une requête HTTP avec Python\">2. Envoyer une requête HTTP avec Python</a>\n",
    "* <a href=\"#Parser une page web avec BeautifulSoup\">3. Parser une page web avec <code>BeautifulSoup</code></a>\n",
    "* <a href=\"#Avec une vraie page web...\">4. Avec une vraie page web...</a>\n",
    "    * <a href=\"#Faire la soupe\">4.1. Faire la soupe !</a>\n",
    "    * <a href=\"#Comprendre la structure de la page web\">4.2. Comprendre la structure de la page web</a>\n",
    "    * <a href=\"#Récupérer les métadonnées\">4.3. Récupérer les métadonnées</a>\n",
    "    * <a href=\"#Tourner la page\">4.4. Tourner la page...</a>\n",
    "    * <a href=\"#Remettons un peu d'ordre\">4.5. Remettons un peu d'ordre</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div id=\"Introduction\">1. Introduction</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons peut-être par définir et expliciter en quoi consiste le **web scraping**. On peut résumer cela en trois points, en empruntant une définition simple proposée par Kimberly Fessel :\n",
    "* Collecter de l'information depuis des sites web\n",
    "* Parser son contenu dans un format structuré que l'on puisse représenter et extraire de la même façon (CSV, JSON, XML, etc.)\n",
    "* Faire le tout de façon entièrement automatisée\n",
    "\n",
    "Se pose également une question plus pratique : pourquoi apprendre à scraper des pages web ? Il y a plusieurs raisons à cela, mais la question a d'autant plus de sens lorsqu'on croise de nombreuses ressources et outils proposant de scraper des sites web sans plonger davantage dans le code. Listons-en quelques-une :\n",
    "* Le web scraping permet de gagner énormément de temps : si l'on souhaite constituer des jeux de données massifs, il devient quasiment inenvisageable de constituer manuellement des tableaux de données de plus de quelques milliers de lignes\n",
    "* C'est une technique qui permet, non pas seulement d'accéder à des données, mais de se constituer soi-même ses propres jeux de données en croisant, associant diverses sources\n",
    "* Elle est une façon de se familiariser avec la construction de données structurées et offre de nombreux concepts et façons de faire transposables à de nombreuses autres opérations faisant intervenir des données structurées (tri de fichiers, catégorisation, tagging, cartographie, etc.)\n",
    "* Enfin, elle constitue un outil d'exploration qui permet de mieux appréhender le fonctionnement des infrastructures numériques contemporaines, en particulier de l'internet et du web\n",
    "\n",
    "On l'aura compris, le **web scraping** vous dote de **super pouvoirs**, mais qui dit grands pouvoirs, dit aussi grandes responsabilités. Ainsi, nous évoquerons les aspects éthiques et légaux du **web scraping** dans le module final de cette formation.\n",
    "\n",
    "Dans ce module, nous passons à la pratique en explorant nos premiers cas concrets d'application de Python pour le **web scraping**. Avant d'entamer ce module, il est vivement conseillé de revoir :\n",
    "* Le module 1 et d'être à l'aise avec le fonctionnement de langages à balises comme HTML\n",
    "* Les module 2 et 3 et d'être à l'aise avec les pratiques élémentaires de programmation avec Python (structures de données, boucles, conditions, etc.)\n",
    "\n",
    "Au cours de ce module, nous apprendrons :\n",
    "* A envoyer une requête HTTP à un serveur à l'aide du module `requests`\n",
    "* A parser une page HTML simple à l'aide du module `BeautifulSoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div id=\"Envoyer une requête HTTP avec Python\">2. Envoyer une requête HTTP avec Python</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accéder à une page web suppose avant tout de la récupérer : pour cela, on utilise le **protocole HTTP** qui nous permet de former des **requêtes HTTP**. L'objectif ici n'est pas de rentrer dans le détail, d'autant que nous avons pu voir quelques aspects élémentaires durant le premier module, que de comprendre simplement les différentes étapes qui structurent un projet de **web scraping**.\n",
    "\n",
    "Python dispose d'une **bibliothèque** nommée `requests` qui permet de construire et d'envoyer des **requêtes HTTP**. Il existe d'autres façons de faire, mais `requests` a l'avantage d'être simple d'utilisation et d'être très flexible et pourra vous être utile pour bien d'autres projets.\n",
    "\n",
    "Voyons toute de suite comment récupérer le contenu d'une page web à l'aide de `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html lang=\"fr-FR\">\n",
      "  <head>\n",
      "      <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\n",
      "    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\n",
      "    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=\n",
      "    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);\n",
      "    })(window,document,'script','dataLayer','GTM-KNCB8CD');</script>\n",
      "  \n",
      "  <meta charset=\"utf-8\">\n",
      "  <meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\">\n",
      "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" X-Content-Type-Options=\"nosniff\">\n",
      "\n",
      "  <link href=\"https://www.msh-lse.fr/wp-content/themes/pamplemousse/dist/images/favicons/apple-touch-icon-57x57.png\" rel=\"apple-touch-icon-precomposed\" sizes=\"57x57\">\n",
      "  <link href=\"https://www.msh-lse.fr/wp-content/themes/pamplemousse/dist/images/favicons/apple-touch-icon-114x114.png\" rel=\"apple-tou\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"https://www.msh-lse.fr/\")\n",
    "# Affichons simplement les 1000 premiers caractères car la page est très longue\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons le temps de commenter ces quelques lignes.\n",
    "\n",
    "Comme avec toute **bibliothèque** de code, il s'agit dans un premier temps de l'importer.\n",
    "\n",
    "Ensuite, on peut utiliser l'**objet** `requests` sur lequel on peut appeler différentes méthodes. Dans notre cas, nous allons utiliser uniquement la **méthode** `.get()` car nous ne nous intéressons qu'à la récupération de pages web. Cette **méthode** `.get()` prend plusieurs **paramètres** et doit contenir, au minimum, un **paramètre** indiquant l'**adresse URL** qui nous intéresse.\n",
    "\n",
    "`requests` renvoie un objet de type `requests.models.Response`. C'est la **réponse** que le serveur nous a renvoyé lorsque nous avons demandé notre **URL** dans la **requête HTTP** que nous lui avons envoyé.\n",
    "\n",
    "Cet **objet** dispose de différentes **propriétés** : la première qui nous intéresse ici est celle nommée `.text`. Elle permet de récupérer l'intégralité du code source de page web (donc du code HTML). A partir de ce code, nous allons pouvoir récupérer les éléments qui nous intéressent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "print(type(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div id=\"Un peu de pratique : Construire et envoyer plusieurs requêtes HTTP\">2. 2. Un peu de pratique : Construire et envoyer plusieurs requêtes HTTP</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\"><strong>Premier exercice : </strong>Dans cet exercice, nous allons créer un seul modèle de requête à l'aide de <code>requests</code> à partir duquel nous effectuerons plusieurs requêtes différentes pour récupérer le code HTML des pages web suivantes :\n",
    "<ul>\n",
    "    <li><a href=\"http://www.reddit.com/r/HTML\">http://www.reddit.com/r/HTML</a></li>\n",
    "    <li><a href=\"http://www.reddit.com/r/python\">http://www.reddit.com/r/python</a></li>\n",
    "    <li><a href=\"http://www.reddit.com/r/webscraping\">http://www.reddit.com/r/webscraping</a></li>\n",
    "</ul>\n",
    "L'objectif est de partir de la liste suivante, qui contient le nom des sub-reddit correspondants : <code>subreddit = [\"html\", \"python\", \"webscraping\"]</code>. On stockera la réponse dans un dictionnaire qui fera correspondre le sub reddit avec le contenu de la page. Si vous vous sentez intrépide, vous pouvez encapsuler le tout dans une fonction pour avoir un code propre et réutilisable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\"><strong>Utiliser la boîte à outil Python pour créer un pipeline de requêtes HTTP :</strong>\n",
    "<br>\n",
    "<br>\n",
    "<pre><code>import requests\n",
    "\n",
    "subreddit = [\"html\", \"python\", \"webscraping\"]\n",
    "content = {}\n",
    "\n",
    "def req(subreddit):\n",
    "    r = requests.get(f\"http://www.reddit.com/r/{subreddit}\")\n",
    "    return r.text\n",
    "\n",
    "for sub in subreddit:\n",
    "    content[sub] = req(sub)</code></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div id=\"Parser une page web avec BeautifulSoup\">3. Parser une page web avec <code>BeautifulSoup</code></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous savons comment récupérer le contenu d'une page web, il ne nous reste plus qu'à apprendre comment **parser** une page web !\n",
    "\n",
    "**Parser** un document signifie le parcourir intégralement et le filtrer afin de ne récupérer que les parties qui nous intéresse. Cela peut être utile lorsque l'on souhaite extraire des données, comme dans notre cas, mais également lorsqu'on souhaite modifier un document, compter des occurrences, etc.\n",
    "\n",
    "`BeautifulSoup` nous aide grandement pour cela. Il y a mille et une façons de s'y prendre pour récupérer les informations qui nous intéressent mais le plus important est d'abord de savoir ce que l'on peut faire et ce que l'on ne peut pas faire. Pour cela, pas de secret, on se dirige à nouveau vers la documentation : https://www.crummy.com/software/BeautifulSoup/bs4/doc/. La chance avec `BeautifulSoup`, c'est que la documentation est particulièrement bien faite et très riche : on trouve de nombreux exemples pour comprendre, en contexte, comment utiliser les méthodes de la bibliothèque.\n",
    "\n",
    "Commençons par voir un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li class=\"email\" id=\"personnel\">wsenter0@blogspot.com</li>, <li class=\"email\" id=\"professionnel\">wsenter0@cyberchimps.com</li>, <li class=\"email\" id=\"personnel\">abroddle1@tripadvisor.com</li>, <li class=\"email\" id=\"professionnel\">abroddle1@wisc.edu</li>, <li class=\"email\" id=\"personnel\">aboothebie2@e-recht24.de</li>, <li class=\"email\" id=\"professionnel\">aboothebie@irs.gov</li>, <li class=\"email\" id=\"personnel\">meast3@webeden.co.uk</li>, <li class=\"email\" id=\"personnel\">meast3@sohu.com</li>, <li class=\"email\" id=\"personnel\">cmcging4@so-net.ne.jp</li>, <li class=\"email\" id=\"professionnel\">cmcging4@people.com.cn</li>]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"data/module_1-carnet.html\", \"r\") as file:\n",
    "    html = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "print(soup.find_all(\"li\", attrs={\"class\": \"email\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de travailler à partir d'une vraie page web, commençons par un petit échauffement avec un fichier HTML très simple (et mal formé, soit dit en pensant, si on se souvient des éléments vus dans le premier module).\n",
    "\n",
    "Le fonctionnement est assez simple : pour **parser** une page web (donc un document HTML) ou un fichier XML avec `BeautifulSoup`, on appelle la fonction `BeautifulSoup()` à laquelle on passe au moins deux paramètres :\n",
    "* Le texte du fichier HTML ou XML\n",
    "* Un parser (il en existe plusieurs, on pourra d'ailleurs se référer à la documentation de `BeautifulSoup` pour avoir plus de détails (https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser), ici nous utiliserons le parser par défaut\n",
    "\n",
    "Une fois que l'on a parsé notre fichier, on peut utiliser toute une série de **méthodes** pour parcourir la structure arborescente de notre fichier HTML ou XML.\n",
    "\n",
    "Dans notre cas, nous utilisons la **méthode** `.find_all()` qui permet de rechercher toutes les balises qui matchent les conditions passées comme **arguments**. On peut passer plusieurs arguments à la **méthode** `.find_all()` :\n",
    "* Le tag à rechercher (`div`, `li`, `a`, etc.)\n",
    "* Un attribut et sa valeur associée dans un dictionnaire que l'on passe au paramètre `attrs`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\"><strong>Deuxième exercice :</strong> A l'aide de <code>BeautifulSoup</code>, récupérez dans un dictionnaire toutes les adresses email des membres du carnet d'adresse. Pour rappel, vous pouvez importer le carnet d'adresse <code>module_1-carnet.html</code> depuis le dossier <code>data</code>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\"><strong>Capturer les erreurs renvoyées par BeautifulSoup</strong>\n",
    "<br>\n",
    "La difficulté dans cet exercice repose sur le fait que BeautifulSoup lève des erreurs lorsqu'il n'y a pas d'attributs pour une balise spécifique. Il faut penser à capturer les erreurs pour que le script ne s'arrête pas en cours de route.\n",
    "<br>\n",
    "<br>\n",
    "<pre><code>from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"data/module_1-carnet.html\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "soup = BeautifulSoup(data, \"html.parser\")\n",
    "carnet = soup.div\n",
    "\n",
    "dict_mail = {}\n",
    "name = \"\"\n",
    "\n",
    "for child in carnet.children:\n",
    "    try:\n",
    "        name = child.li.get_text()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    try:\n",
    "        dict_mail[name] = child.find_all(\"li\", attrs={\"class\": \"email\"})\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(dict_mail)</code></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div id=\"Avec une vraie page web...\">4. Avec une vraie page web...</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les choses vont un peu se corser maintenant que l'on comprend les principes de base : nous allons passer à un exemple \"réel\" en scrapant une véritable page web.\n",
    "\n",
    "Nous allons tenter de récupérer les informations de l'historique de l'émission \"Le cours de l'histoire\" sur le site internet de France Culture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div id=\"#Faire la soupe\">4. 1. Faire la soupe !</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par nous connecter à la page à partir de laquelle on souhaite travailler et affichons les 100 premiers caractères de la page web pour vérifier que l'on a bel et bien récupéré une page web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n    <html lang=\"fr\" prefix=\"og: http://ogp.me/ns#\">\\n    <head>\\n                     '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"https://www.franceculture.fr/emissions/le-cours-de-lhistoire\")\n",
    "r.text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous pouvons créer une **instance** de `BeautifulSoup` en passant la page web au format texte en que l'on a récupéré à l'aide de `requests` en **paramètre**.\n",
    "\n",
    "On pensera également à ajouter en **second paramètre** un **parser** pour parcourir la page web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div id=\"#Comprendre la structure de la page web\">4. 2. Comprendre la structure de la page web</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, il nous faut être un peu malins et essayer de comprendre, en consultant les **outils de développement de notre navigateur préféré**, à partir de quelle **balise** on peut opérer pour récupérer les données qui nous intéressent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/france_culture_balise.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ici que toutes les émissions sont contenues dans la **balise** `<div class=\"teaser-list\">`. On peut donc se positionner à se niveau-là et récupérer toutes les **balises enfants**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = soup.find(\"div\", attrs={\"class\": \"teasers-list\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En observant la structure du code HTML, on constate que les informations sur chaque épisode sont contenues dans les **balises** `<div class=\"teaser teaser-row teaser-expression\">`.\n",
    "\n",
    "Si on utilise la propriété `.children` de `BeautifulSoup`, on peut directement se positionner au niveau de ces **balises enfants** pour toutes les parcourir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/france_culture_balises_enfants.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate alors que ces **balises** contiennent à leur tour des **balises enfants** qui contiennent les métadonnées de chaque épisode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On parcourt toutes les sous-balises de la balise parent <div class=\"teaser-list\">\n",
    "for item in div.children:\n",
    "    # On cherche les balises <div class=\"teaser-text\"> qui contiennent les métadonnées\n",
    "    teaser_text = item.find(\"div\", attrs={\"class\": \"teaser-text\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notons d'emblée qu'il y **15 balises enfants** `<div class=\"teaser teaser-row teaser-expression\">`.\n",
    "\n",
    "Si on prend le temps d'observer ce qu'il se passe dans notre boucle, on se rendre compte qu'il y a peut-être certaines balises qui ne nous intéressent pas...\n",
    "\n",
    "Déployons un petit stratagème pour voir efficacement le contenu des balises que l'on parcourt en itérant sur les **balises enfants** de notre **balise** `<div class=\"teaser-list>`.\n",
    "\n",
    "Notre **variable** `teaser_text` n'est pas une donnée de type **chaîne de caractère**, mais on peut essayer de la convertir en une **chaîne de caractère** à l'aide de la **fonction** `str()`.\n",
    "\n",
    "Si on veut, on peut aussi **incrémenter** un compteur (`i`) à chaque tour de boucle pour compter le **nombre de balises enfants** que récupère `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"hidden\" data-next-episodes=\"{&quot;6911f504-b143-45ad-b21a-b6ed0f4ec146&quot;:\n",
      "<link href=\"https://www.franceculture.fr/emissions/le-cours-de-lhistoire?p=2\" rel=\"next\"/>\n",
      "<link href=\"https://www.franceculture.fr/emissions/le-cours-de-lhistoire\" rel=\"first\"/>\n",
      "<link href=\"https://www.franceculture.fr/emissions/le-cours-de-lhistoire?p=22\" rel=\"last\"/\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"teaser teaser-row teaser-expression\"><div class=\"teaser-image-container\"><a cl\n",
      "<div class=\"pager-container\"><ul class=\"pager visible margin-bottom\"><li class=\"pager-item\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for item in div.children:\n",
    "    teaser_text = item.find(\"div\", attrs={\"class\": \"teaser-text\"})\n",
    "    print(str(item)[:90])\n",
    "    i += 1\n",
    "    \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut constater qu'il y a certaines **balises enfants** qui ne nous intéressent pas vraiment dans le lot ! Il faudra les gérer dans la suite de notre code pour avoir des données propres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div id=\"#Récupérer les métadonnées\">4. 3. Récupérer les métadonnées</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le plus intéressant, à ce stade, serait de créer un **dictionnaire** dans lequel on puisse stocker chaque métadonnée qui nous intéresse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_metadata = {\"title\": \"\",\n",
    "                 \"date\": \"\",\n",
    "                 \"duration\": \"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que l'on a notre **dictionnaire**, on peut le remplir à mesure que l'on passe sur chaque épisode pour récupérer leurs **métadonnées**. Néanmoins, il nous faudra stocker chaque dictionnaire représentant un épisode dans une **liste**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_metadata = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in div.children:\n",
    "    teaser_text = item.find(\"div\", attrs={\"class\": \"teaser-text\"})\n",
    "    \n",
    "    # On récupère le titre de l'émission\n",
    "    try:\n",
    "        title = teaser_text.find(\"div\", attrs={\"class\": \"teaser-text-title\"})\n",
    "        title = title.get_text()\n",
    "        dict_metadata[\"title\"] = title\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # On récupère la date de l'émission\n",
    "    try:\n",
    "        date = teaser_text.find(\"div\", attrs={\"class\": \"teaser-text-date\"})\n",
    "        date = date.get_text()\n",
    "        dict_metadata[\"date\"] = date\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    teaser_img = item.find(\"div\", attrs={\"class\": \"teaser-image-container\"})\n",
    "    \n",
    "    # On récupère la durée de l'épisode\n",
    "    try:\n",
    "        duration = teaser_img.find(\"span\", attrs={\"class\": \"replay-button-duration\"})\n",
    "        # On filtre uniquement le texte\n",
    "        duration = duration.get_text()\n",
    "        # On ajoute la donnée au dictionnaire\n",
    "        dict_metadata[\"duration\"] = duration\n",
    "    except:\n",
    "        continue\n",
    "            \n",
    "    # A chaque tour de boucle, on ajoute la liste des métadonnées d'un épisode\n",
    "    list_metadata.append(dict_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Épisode 2 : Le mardi, c’est spaghettis (antiques) ',\n",
       "  'date': 'LE\\n                            08/12/2020',\n",
       "  'duration': '51 min'},\n",
       " {'title': 'Épisode 2 : Le mardi, c’est spaghettis (antiques) ',\n",
       "  'date': 'LE\\n                            08/12/2020',\n",
       "  'duration': '51 min'},\n",
       " {'title': 'Épisode 2 : Le mardi, c’est spaghettis (antiques) ',\n",
       "  'date': 'LE\\n                            08/12/2020',\n",
       "  'duration': '51 min'},\n",
       " {'title': 'Épisode 2 : Le mardi, c’est spaghettis (antiques) ',\n",
       "  'date': 'LE\\n                            08/12/2020',\n",
       "  'duration': '51 min'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_metadata[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oups... Problème : toutes les métadonnées sont les mêmes ! La raison est très simple, à chaque tour de boucle, nous n'avons pas ajouté le nouveau dictionnaire mais une **référence au dictionnaire**. Pour que les métadonnées soient bien modifiées dans notre `list_metadata`, nous avons deux options :\n",
    "* Ecraser le dictionnaire à chaque nouveau tour de boucle et en créer un nouveau\n",
    "* Créer une copie du dictionnaire et y injecter les nouvelles valeurs\n",
    "\n",
    "Avant de continuer, pensons à écraser notre liste `list_metadata` pour pas récupérer les doublons issus de notre première boucle..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_metadata = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in div.children:\n",
    "    teaser_text = item.find(\"div\", attrs={\"class\": \"teaser-text\"})\n",
    "    \n",
    "    metadata = dict_metadata.copy()\n",
    "    \n",
    "    # On récupère le titre de l'émission\n",
    "    try:\n",
    "        title = teaser_text.find(\"div\", attrs={\"class\": \"teaser-text-title\"})\n",
    "        title = title.get_text()\n",
    "        metadata[\"title\"] = title\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    # On récupère la date de l'émission\n",
    "    try:\n",
    "        date = teaser_text.find(\"div\", attrs={\"class\": \"teaser-text-date\"})\n",
    "        date = date.get_text()\n",
    "        metadata[\"date\"] = date\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    teaser_img = item.find(\"div\", attrs={\"class\": \"teaser-image-container\"})\n",
    "    \n",
    "    # On récupère la durée de l'épisode\n",
    "    try:\n",
    "        duration = teaser_img.find(\"span\", attrs={\"class\": \"replay-button-duration\"})\n",
    "        # On filtre uniquement le texte\n",
    "        duration = duration.get_text()\n",
    "        # On ajoute la donnée au dictionnaire\n",
    "        metadata[\"duration\"] = duration\n",
    "    except:\n",
    "        continue\n",
    "            \n",
    "    # A chaque tour de boucle, on ajoute la liste des métadonnées d'un épisode\n",
    "    list_metadata.append(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Épisode 1 : Futurs antérieurs, histoire des représentations de l’avenir',\n",
       "  'date': 'LE\\n                            04/01/2021',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Une girafe à Paris : le voyage de Zarafa',\n",
       "  'date': 'LE\\n                            01/01/2021',\n",
       "  'duration': '53 min'},\n",
       " {'title': 'Épisode 4 :  Mussolini peut-il être un personnage de roman ?',\n",
       "  'date': 'LE\\n                            24/12/2020',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Épisode 3 : L’historien à la manette, quand le jeu vidéo raconte l’histoire',\n",
       "  'date': 'LE\\n                            23/12/2020',\n",
       "  'duration': '52 min'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_metadata[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ça a l'air pas mal !\n",
    "\n",
    "Reste un problème : la date n'est pas formatée très proprement, l'idéal serait de la retravailler à l'aide d'un **regex**.\n",
    "\n",
    "Deux options là aussi :\n",
    "* Le retravailler tout de suite\n",
    "* Nettoyer le jeu de données à l'issue de la collecte\n",
    "\n",
    "Dans notre cas, nous allons continuer et nous nettoierons le jeu de données à la fin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div id=\"Tourner la page\">4. 4. Tourner la page...</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, nous avons récupéré toutes les métadonnées des émissions pour la première page. Maintenant, ce que l'on voudrait faire, c'est passer à la page suivante et répéter la même opération.\n",
    "\n",
    "Premier réflexe : si on souhaite répéter la même opération, il serait judicieux d'encapsuler notre code dans une **fonction** pour pouvoir le réutiliser.\n",
    "\n",
    "Commençons par ça."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(soup):\n",
    "    \n",
    "    # On se positionne au niveau de la balise parent qui nous intéresse\n",
    "    div = soup.find(\"div\", attrs={\"class\": \"teasers-list\"})\n",
    "    \n",
    "    # On crée le schéma de métadonnées à l'aide d'un dictionnaire\n",
    "    dict_metadata = {\"title\": \"\",\n",
    "                     \"date\": \"\",\n",
    "                     \"duration\": \"\"}\n",
    "    \n",
    "    # On crée une liste de vide pour ajouter à chaque tour de boucle un nouveau dictionnaire de métadonnées\n",
    "    list_metadata = []\n",
    "    \n",
    "    for item in div.children:\n",
    "        teaser_text = item.find(\"div\", attrs={\"class\": \"teaser-text\"})\n",
    "\n",
    "        metadata = dict_metadata.copy()\n",
    "\n",
    "        # On récupère le titre de l'émission\n",
    "        try:\n",
    "            title = teaser_text.find(\"div\", attrs={\"class\": \"teaser-text-title\"})\n",
    "            title = title.get_text()\n",
    "            metadata[\"title\"] = title\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # On récupère la date de l'émission\n",
    "        try:\n",
    "            date = teaser_text.find(\"div\", attrs={\"class\": \"teaser-text-date\"})\n",
    "            date = date.get_text()\n",
    "            metadata[\"date\"] = date\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        teaser_img = item.find(\"div\", attrs={\"class\": \"teaser-image-container\"})\n",
    "\n",
    "        # On récupère la durée de l'épisode\n",
    "        try:\n",
    "            duration = teaser_img.find(\"span\", attrs={\"class\": \"replay-button-duration\"})\n",
    "            # On filtre uniquement le texte\n",
    "            duration = duration.get_text()\n",
    "            # On ajoute la donnée au dictionnaire\n",
    "            metadata[\"duration\"] = duration\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # A chaque tour de boucle, on ajoute la liste des métadonnées d'un épisode\n",
    "        list_metadata.append(metadata)\n",
    "        \n",
    "    return list_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parfait, maintenant, nous avons une fonction qui permet de retourner une **liste de métadonnées**. On pourra l'utiliser pour chaque page que nous allons parcourir.\n",
    "\n",
    "Pour comprendre comment passer à la page suivante, rien de plus simple : cliquons sur le bouton !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/france_culture_nextpage.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En passant à la page suivante, on constate que l'**URL** de la page web est différente et elle semble suivre une structure cohérente :\n",
    "1. `www.franceculture.fr/emissions/le-cours-de-lhistoire`\n",
    "2. `www.franceculture.fr/emissions/le-cours-de-lhistoire?p=2`\n",
    "3. `www.franceculture.fr/emissions/le-cours-de-lhistoire?p=3`\n",
    "4. `www.franceculture.fr/emissions/le-cours-de-lhistoire?p=4`\n",
    "\n",
    "Etc.\n",
    "\n",
    "Rien de plus simple pour nous ! Il nous suffit de modifier l'URL en incrémenter la page de 1 à après que notre fonction `scrape_page()` a fini de travailler. Il nous faudra simplement tenir compte de la première page qui est un peu différente.\n",
    "\n",
    "Voyons cela tout de suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.franceculture.fr/emissions/le-cours-de-lhistoire\"\n",
    "\n",
    "# On récupère la page web\n",
    "r = requests.get(url)\n",
    "# On fait la soupe...\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "# On utilise notre fonction pour scraper la page (on se rappelle qu'elle retourne une liste de métadonnées)\n",
    "data = scrape_page(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Épisode 1 : Futurs antérieurs, histoire des représentations de l’avenir',\n",
       "  'date': 'LE\\n                            04/01/2021',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Une girafe à Paris : le voyage de Zarafa',\n",
       "  'date': 'LE\\n                            01/01/2021',\n",
       "  'duration': '53 min'},\n",
       " {'title': 'Épisode 4 :  Mussolini peut-il être un personnage de roman ?',\n",
       "  'date': 'LE\\n                            24/12/2020',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Épisode 3 : L’historien à la manette, quand le jeu vidéo raconte l’histoire',\n",
       "  'date': 'LE\\n                            23/12/2020',\n",
       "  'duration': '52 min'},\n",
       " {'title': \"Épisode 2 : L'Histoire dessinée, histoire de buller\",\n",
       "  'date': 'LE\\n                            22/12/2020',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Épisode 1 : Quand l’histoire rebat les cartes ',\n",
       "  'date': 'LE\\n                            21/12/2020',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Il y a cent ans, la naissance du Parti communiste français ',\n",
       "  'date': 'LE\\n                            18/12/2020',\n",
       "  'duration': '51 min'},\n",
       " {'title': 'Épisode 4 : \"N’oublie pas mon petit soulier\" : une histoire du cadeau de Noël ',\n",
       "  'date': 'LE\\n                            17/12/2020',\n",
       "  'duration': '51 min'},\n",
       " {'title': 'Épisode 3 : Joue, grandis, apprends : quand la cour de récré devient la salle de classe',\n",
       "  'date': 'LE\\n                            16/12/2020',\n",
       "  'duration': '51 min'},\n",
       " {'title': 'Épisode 2 : La stratégie sur un plateau, aux origines du jeu de société',\n",
       "  'date': 'LE\\n                            15/12/2020',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Épisode 1 : Loteries royales, les jeux de l’État et du hasard',\n",
       "  'date': 'LE\\n                            14/12/2020',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Staline et le Livre noir, la persécution des Juifs d’URSS',\n",
       "  'date': 'LE\\n                            11/12/2020',\n",
       "  'duration': '51 min'},\n",
       " {'title': 'Épisode 4 : Jeudi, c’est méchoui (et couscous aussi)',\n",
       "  'date': 'LE\\n                            10/12/2020',\n",
       "  'duration': '51 min'},\n",
       " {'title': 'Épisode 3 : Le mercredi, c’est sushi (une tranche d’histoire)',\n",
       "  'date': 'LE\\n                            09/12/2020',\n",
       "  'duration': '51 min'},\n",
       " {'title': 'Épisode 2 : Le mardi, c’est spaghettis (antiques) ',\n",
       "  'date': 'LE\\n                            08/12/2020',\n",
       "  'duration': '51 min'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incroyable, tout marche à merveille !\n",
    "\n",
    "Maintenant, il nous faut donc gérer nos pages suivantes et faire attention à la façon dont on ajoute les métadonnées des épisodes suivants dans la liste `data`.\n",
    "\n",
    "On se rappelle qu'on peut ajouter les éléments d'une liste à une seconde liste simplement avec l'opérateur `+`. On optera pour cette solution, certes peu élégante, mais fonctionnelle pour l'instant. Une fois que l'on sera assuré d'avoir bien récupérer les données, on pourra modifier un peu notre code pour faire quelque chose d'un peu plus propre.\n",
    "\n",
    "En attendant, voyons comment transformer notre **URL** pour passer au scraping des pages suivantes. Pour cela, rien de plus simple : on peut simplement utiliser une **string formatée** que l'on fera évoluer à chaque tour de boucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.franceculture.fr/emissions/le-cours-de-lhistoire?p=2'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = 2\n",
    "url_page = f\"?p={page}\"\n",
    "url + url_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinons tout cela maintenant. Mais pensons à vérifier avant combien il y a de pages au total (spoiler : il y a en 22).\n",
    "\n",
    "Cela peut prendre un peu de temps, il y a quand même un peu de boulot... Si on veut pouvoir suivre la progression du travail, on peut utiliser une **bibliothèque** au petit nom de `tqdm`. En entourant un **itérable** de la fonction `tqdm()`, on peut suivre la progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:06<00:00,  3.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "url = \"https://www.franceculture.fr/emissions/le-cours-de-lhistoire\"\n",
    "\n",
    "# On récupère la page web\n",
    "r = requests.get(url)\n",
    "# On fait la soupe...\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "# On utilise notre fonction pour scraper la page (on se rappelle qu'elle retourne une liste de métadonnées)\n",
    "data = scrape_page(soup)\n",
    "\n",
    "# Nombre de pages à tourner\n",
    "nb_page = 22\n",
    "# Page de départ\n",
    "page = 2\n",
    "\n",
    "for i in tqdm(range(2, nb_page+1)):\n",
    "    page = i\n",
    "    new_url = url + f\"?p={page}\"\n",
    "    r = requests.get(new_url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    d = scrape_page(soup)\n",
    "    data.extend(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir qu'on a récupéré les métadonnées pour 327 épisodes. C'est pas mal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ordre des dates a l'air décroissant, on dirait qu'on a bien réussi à récupérer ce qu'on voulait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Épisode 2 : Comment les nationalismes puisent dans les civilisations perdues ou inventées pour se construire ? ',\n",
       "  'date': 'LE\\n                            03/09/2019',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Épisode 1 : Pourquoi les paradis perdus nous fascinent-ils tant ?',\n",
       "  'date': 'LE\\n                            02/09/2019',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Épisode 5 : Berlin à la trace  ',\n",
       "  'date': 'LE\\n                            30/08/2019',\n",
       "  'duration': '53 min'},\n",
       " {'title': 'Épisode 4 : La querelle des images ou l’art allemand après la chute du mur de Berlin',\n",
       "  'date': 'LE\\n                            29/08/2019',\n",
       "  'duration': '53 min'},\n",
       " {'title': 'Épisode 3 : Trente ans après, comment agissent encore les archives de la Stasi sur la société allemande ?',\n",
       "  'date': 'LE\\n                            28/08/2019',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Épisode 2 : La RDA par elle-même',\n",
       "  'date': 'LE\\n                            27/08/2019',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Épisode 1 : Un mur, trois révolutions, comment les événements de 1989 ont-ils changé la façon d’écrire l’histoire ?',\n",
       "  'date': 'LE\\n                            26/08/2019',\n",
       "  'duration': '52 min'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[320:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div id=\"Remettons un peu d'ordre\">4. 5. Remettons un peu d'ordre</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon, c'est pas mal, on a un code fonctionnel, mais le mieux serait d'avoir un code qui soit à la fois fonctionnel et propre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:07<00:00,  2.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Nombre de pages à tourner\n",
    "nb_page = 22\n",
    "\n",
    "for page in tqdm(range(nb_page + 1)):\n",
    "    \n",
    "    if page < 2:\n",
    "        url = \"https://www.franceculture.fr/emissions/le-cours-de-lhistoire\"\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        data = scrape_page(soup)\n",
    "    else:\n",
    "        new_url = url + f\"?p={page}\"\n",
    "        r = requests.get(new_url)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        new_data = scrape_page(soup)\n",
    "        data.extend(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est un peu mieux. Mais rappelons-nous qu'il nous reste à nettoyer un peu les données que l'on a collecté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LE\\n                            04/01/2021'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas incroyable. Essayons de voir si on peut arranger cela avec une **regex**.\n",
    "\n",
    "Dans la date que l'on récupère, on a un motif qui se répète :\n",
    "* \"LE\"\n",
    "* \"\\n\"\n",
    "* Plein d'espaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04/01/2021'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# On matche LE et tous les espaces (y compris les tabulations et les retour chariot)\n",
    "regex = re.compile(r\"LE\\s+\")\n",
    "\n",
    "result = re.sub(regex, \"\", data[0][\"date\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut encapsuler ça dans une fonction et nettoyer notre jeu de données grâce à elle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date(date):\n",
    "    \n",
    "    regex = re.compile(r\"LE\\s+\")\n",
    "    result = re.sub(regex, \"\", date)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allons-y pour faire passer nos données dans la fonction `clean_date()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in data:\n",
    "    elem[\"date\"] = clean_date(elem[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Épisode 1 : Futurs antérieurs, histoire des représentations de l’avenir',\n",
       "  'date': '04/01/2021',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Une girafe à Paris : le voyage de Zarafa',\n",
       "  'date': '01/01/2021',\n",
       "  'duration': '53 min'},\n",
       " {'title': 'Épisode 4 :  Mussolini peut-il être un personnage de roman ?',\n",
       "  'date': '24/12/2020',\n",
       "  'duration': '52 min'},\n",
       " {'title': 'Épisode 3 : L’historien à la manette, quand le jeu vidéo raconte l’histoire',\n",
       "  'date': '23/12/2020',\n",
       "  'duration': '52 min'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ça a l'air pas mal ! Chargeons toutes les données dans un fichier CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Épisode 1 : Futurs antérieurs, histoire des re...</td>\n",
       "      <td>04/01/2021</td>\n",
       "      <td>52 min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Une girafe à Paris : le voyage de Zarafa</td>\n",
       "      <td>01/01/2021</td>\n",
       "      <td>53 min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Épisode 4 :  Mussolini peut-il être un personn...</td>\n",
       "      <td>24/12/2020</td>\n",
       "      <td>52 min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Épisode 3 : L’historien à la manette, quand le...</td>\n",
       "      <td>23/12/2020</td>\n",
       "      <td>52 min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Épisode 2 : L'Histoire dessinée, histoire de b...</td>\n",
       "      <td>22/12/2020</td>\n",
       "      <td>52 min</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title        date duration\n",
       "0  Épisode 1 : Futurs antérieurs, histoire des re...  04/01/2021   52 min\n",
       "1           Une girafe à Paris : le voyage de Zarafa  01/01/2021   53 min\n",
       "2  Épisode 4 :  Mussolini peut-il être un personn...  24/12/2020   52 min\n",
       "3  Épisode 3 : L’historien à la manette, quand le...  23/12/2020   52 min\n",
       "4  Épisode 2 : L'Histoire dessinée, histoire de b...  22/12/2020   52 min"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"cours_histoire.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources et références utiles\n",
    "\n",
    "Le présent support a été réalisé à l'aide des références suivantes.\n",
    "\n",
    "* **Documentation officielle :**\n",
    "    * **BeautifulSoup, URL : https://www.crummy.com/software/BeautifulSoup/bs4/doc/.**\n",
    "    * **Requests, URL : https://requests.readthedocs.io/en/master/.**\n",
    "* **Tutoriel vidéo :**\n",
    "    * **Kimberly Fessel, \"It's officially legal so let's scrape the web\", URL : https://www.youtube.com/watch?v=RUQWPJ1T6Zc**. : Un tutoriel très complet qui part des bases et illustre toutes les étapes nécessaires pour scraper les données d'un site web."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
